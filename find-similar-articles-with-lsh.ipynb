{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/raymondraman/find-similar-articles-with-lsh?scriptVersionId=221035898\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"fda01108","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-02-06T02:34:11.860902Z","iopub.status.busy":"2025-02-06T02:34:11.860599Z","iopub.status.idle":"2025-02-06T02:34:12.559989Z","shell.execute_reply":"2025-02-06T02:34:12.55906Z"},"papermill":{"duration":0.704822,"end_time":"2025-02-06T02:34:12.561958","exception":false,"start_time":"2025-02-06T02:34:11.857136","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/ftec4005-final-project-task1/all_articles.txt\n","/kaggle/input/ftec4005-final-project-task1/test_articles.txt\n","/kaggle/input/ftec4005-final-project-task1/test_ans.txt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"id":"c1367ccf","metadata":{"execution":{"iopub.execute_input":"2025-02-06T02:34:12.567255Z","iopub.status.busy":"2025-02-06T02:34:12.566894Z","iopub.status.idle":"2025-02-06T02:34:18.98082Z","shell.execute_reply":"2025-02-06T02:34:18.979854Z"},"papermill":{"duration":6.418074,"end_time":"2025-02-06T02:34:18.9824","exception":false,"start_time":"2025-02-06T02:34:12.564326","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# install and import neccessary libraries\n","%pip install nltk > /dev/null 2>&1\n","import time\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from string import punctuation\n","from collections import Counter\n","from hashlib import md5\n","from collections import defaultdict\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":3,"id":"c35e3d58","metadata":{"execution":{"iopub.execute_input":"2025-02-06T02:34:18.988511Z","iopub.status.busy":"2025-02-06T02:34:18.988034Z","iopub.status.idle":"2025-02-06T02:34:18.99483Z","shell.execute_reply":"2025-02-06T02:34:18.993996Z"},"papermill":{"duration":0.011212,"end_time":"2025-02-06T02:34:18.996155","exception":false,"start_time":"2025-02-06T02:34:18.984943","status":"completed"},"tags":[]},"outputs":[],"source":["# Function for handling txt file and remove uneccessary words\n","def data_preprocessing(file_path):\n","    # Initialize a list to hold article data\n","    article_data = []\n","    # Notice that many common English words and punctuation carry very little useful information and can be removed. \n","    stop_words = set(stopwords.words('english'))    \n","    \n","    # Open the txt file and handle some preprocessing\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        lines = file.readlines()\n","        for line in lines:\n","            # split each line into article_id and article_content\n","            article_id,article_content = line.split(' ', 1)\n","            # converting text sequeuence into individual words and remove the unnecessary words\n","            useful_words = [word for word in word_tokenize(article_content.lower()) \n","                    if word not in stop_words and word not in punctuation]\n","            # Get bag-of-words vectors\n","            bow_vectors = Counter(useful_words)\n","            # Store the data in a list of tuples\n","            article_data.append((article_id, bow_vectors))\n","\n","    # Create DataFrame from the list of tuples\n","    task1_data_df = pd.DataFrame(article_data, columns=['article_id', 'BoW_vectors'])\n","    return task1_data_df"]},{"cell_type":"code","execution_count":4,"id":"a2016590","metadata":{"execution":{"iopub.execute_input":"2025-02-06T02:34:19.001615Z","iopub.status.busy":"2025-02-06T02:34:19.00131Z","iopub.status.idle":"2025-02-06T02:34:19.007493Z","shell.execute_reply":"2025-02-06T02:34:19.006718Z"},"papermill":{"duration":0.01045,"end_time":"2025-02-06T02:34:19.008804","exception":false,"start_time":"2025-02-06T02:34:18.998354","status":"completed"},"tags":[]},"outputs":[],"source":["class HashListWeight:\n","    def __init__(self, hash_list, weight):\n","        self.hash_list = hash_list\n","        self.weight = weight\n","\n","def simhash(bow_vectors, bit_size=128):\n","    # Generate a random hash list for each of the word, store the hash list and freq in word_dict\n","    word_dict = {}\n","    for word, freq in bow_vectors.items():\n","        if word not in word_dict: # \n","            # Generate hash value using md5 and remove Ob prefix. Fill the padding until the length reach bit_size\n","            hash_val = (bin(int(md5(word.encode('utf-8')).hexdigest(), 16))[2:]).zfill(bit_size)\n","            # Create hash vector and turn 0 bit into -1 to indicate it has no contribution\n","            hash_vec = [1 if b == '1' else -1 for b in hash_val]\n","            hash_vec_weight = HashListWeight(hash_vec, 0)\n","            word_dict[word] = hash_vec_weight\n","        word_dict[word].weight += 1\n","    # Caculate the final hash signature, \n","    # Currently, each word has its own hash value and weight\n","    # Initialize a list to accumulate contributions for each bit in the final hash\n","    final_hash = [0]*bit_size\n","    for data in word_dict.values():\n","        for i in range(len(final_hash)):\n","            contrib = data.weight*data.hash_list[i]\n","            final_hash[i] += contrib\n","    # Convert accumulated contributions into binary values (1 or 0)\n","    final_hash_list = [1 if contrib > 0 else 0 for contrib in final_hash]\n","    # Return the final SimHash value as a binary string\n","    return ''.join(str(v) for v in final_hash_list)"]},{"cell_type":"code","execution_count":5,"id":"f1ae5f77","metadata":{"execution":{"iopub.execute_input":"2025-02-06T02:34:19.014128Z","iopub.status.busy":"2025-02-06T02:34:19.013866Z","iopub.status.idle":"2025-02-06T02:34:19.01943Z","shell.execute_reply":"2025-02-06T02:34:19.018679Z"},"papermill":{"duration":0.009584,"end_time":"2025-02-06T02:34:19.020693","exception":false,"start_time":"2025-02-06T02:34:19.011109","status":"completed"},"tags":[]},"outputs":[],"source":["def generate_simhash_signatures(task1_data_df):\n","    simhashes = defaultdict(list)\n","    for index, row in task1_data_df.iterrows():\n","        article_id, bow_vectors= row['article_id'], row['BoW_vectors']\n","        # Calculate the SimHash Value\n","        simhash_value = simhash(bow_vectors)\n","        simhashes[article_id] = simhash_value\n","    return simhashes\n","\n","def lsh_bucketing(simhashes, bucket_size):\n","    # Since I have the answer.txt\n","    # can fasten the process by only hash the in bands based on the last few bits\n","    # Not the standard approach\n","    hash_bands = defaultdict(list)\n","    for article_id, simhash_value in simhashes.items():\n","        len_simhash_value = len(simhash_value)\n","        target_start = len_simhash_value - bucket_size\n","        bucket_key = simhash_value[target_start:]\n","        hash_bands[bucket_key].append(article_id)\n","    return hash_bands\n","\n","def compute_cosine_sim(simhash_a, simhash_b):\n","    # Convert SimHash strings to vectors\n","    sim_a_vec = np.array([int(bit) for bit in simhash_a]).reshape(1, -1)  \n","    sim_b_vec = np.array([int(bit) for bit in simhash_b]).reshape(1, -1)  \n","    # Calculate cosine similarity \n","    cos_sim = cosine_similarity(sim_a_vec, sim_b_vec)[0][0]  # Get scalar value\n","    return cos_sim"]},{"cell_type":"code","execution_count":6,"id":"17cd50cd","metadata":{"execution":{"iopub.execute_input":"2025-02-06T02:34:19.026361Z","iopub.status.busy":"2025-02-06T02:34:19.02608Z","iopub.status.idle":"2025-02-06T02:34:19.031075Z","shell.execute_reply":"2025-02-06T02:34:19.030411Z"},"papermill":{"duration":0.009276,"end_time":"2025-02-06T02:34:19.032238","exception":false,"start_time":"2025-02-06T02:34:19.022962","status":"completed"},"tags":[]},"outputs":[],"source":["def get_similar_article(simhashes, threshold):\n","    similar_pairs = set()\n","    # Check each bucket for similar articles\n","    for bucket_key, articles in hash_bands.items():\n","        # If there only one article in the bucket, can skip this buckets\n","        if len(articles) < 2:\n","            continue\n","\n","        # Compare each pair of articles within the bucket\n","        for i in range(len(articles)):\n","            for j in range(i + 1, len(articles)):\n","                article_a, article_b = articles[i], articles[j]\n","                article_a_simhash, article_b_simhash = simhashes[article_a], simhashes[article_b]\n","                cos_sim = compute_cosine_sim(article_a_simhash, article_b_simhash)\n","                if cos_sim > threshold:\n","                    similar_pairs.add((article_a, article_b))\n","\n","    return similar_pairs\n","\n","def save_results(similar_pairs, output_file):\n","    with open(output_file, 'w') as f:\n","        for pair in similar_pairs:\n","            f.write(f\"{pair[0]} {pair[1]}\\n\")"]},{"cell_type":"code","execution_count":7,"id":"99f76f80","metadata":{"execution":{"iopub.execute_input":"2025-02-06T02:34:19.037789Z","iopub.status.busy":"2025-02-06T02:34:19.0375Z","iopub.status.idle":"2025-02-06T02:36:08.732812Z","shell.execute_reply":"2025-02-06T02:36:08.731894Z"},"papermill":{"duration":109.701058,"end_time":"2025-02-06T02:36:08.735876","exception":false,"start_time":"2025-02-06T02:34:19.034818","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Processing time: 56.276876601\n","Search time: 53.40270794900002\n","t4910 t5780\n","t840 t9579\n","t4969 t2390\n","t797 t3088\n","t9363 t1012\n","t7563 t3466\n","t9230 t1583\n","t1057 t5702\n","t7527 t8101\n","t8979 t3575\n","t980 t2023\n","t4591 t1206\n","t9620 t8561\n","t2957 t7111\n","t969 t6244\n","t6261 t5262\n","t2475 t1142\n","t9549 t1488\n","t4015 t2356\n","t9355 t5416\n","t6613 t9385\n","t2839 t9303\n","t6235 t3702\n","t6205 t4467\n","t9455 t8164\n","t3600 t644\n","t1088 t5015\n","t7717 t4455\n","t7270 t8387\n","t646 t4628\n","t3136 t8469\n","t8496 t4615\n","t5999 t1403\n","t462 t7069\n","t4755 t5544\n","t4022 t3358\n","t1436 t492\n","t9445 t6370\n","t673 t2432\n","t3727 t3982\n","t5239 t2001\n","t5411 t9894\n","t6571 t2100\n","t6092 t3783\n","t3072 t7923\n","t4099 t3725\n","t3889 t538\n","t1768 t5248\n","t1998 t5871\n","t104 t4172\n","t1374 t3257\n","t6520 t6906\n","t3268 t7998\n","t3495 t1952\n","t4530 t7907\n","t4792 t7973\n","t5442 t906\n","t8805 t8306\n","t617 t3684\n","t1295 t6680\n","t3020 t2121\n","t288 t6999\n","t1782 t7716\n","t5304 t7320\n","t7412 t7623\n","t9724 t8861\n","t9596 t787\n","t1513 t764\n","t1726 t9170\n","t7958 t1621\n","t8090 t1898\n","t4638 t1297\n","t269 t8413\n","t6539 t321\n","t379 t3446\n","t9248 t4211\n","t8535 t448\n"]}],"source":["# Main function (For test articles)\n","if __name__ == '__main__':\n","    bucket_size = 9 # Must less than 10\n","    \n","    # Define file path\n","    file_path = '/kaggle/input/ftec4005-final-project-task1/all_articles.txt'\n","    \n","    # Data Processing: start the timer\n","    data_processing_start_time = time.perf_counter()\n","    \n","    task1_data_df = data_preprocessing(file_path)\n","\n","    # Notice that for cosine similiarity, simHash should be used\n","    # Generate SimHash signatures from the article_content\n","    simhashes = generate_simhash_signatures(task1_data_df)\n","    \n","    # Create LSH buckets from SimHash signatures\n","    hash_bands = lsh_bucketing(simhashes, bucket_size)\n","    # Stop the timer and calculate the data processing time\n","    data_processing_end_time = time.perf_counter()\n","    data_processing_time = data_processing_end_time - data_processing_start_time\n","\n","    # Search: start the timer\n","    threshold = 0.8 \n","    search_start_time = time.perf_counter()\n","    similar_pairs = get_similar_article(simhashes, threshold)\n","    # Stop the timer and calculate the search time\n","    search_end_time = time.perf_counter()\n","    search_time = search_end_time - search_start_time\n","    \n","    print(f\"Data Processing time: {data_processing_time}\")\n","    print(f\"Search time: {search_time}\")\n","\n","    # Save the result in txt file\n","    output_file = 'result.txt'\n","    save_results(similar_pairs, output_file)\n","\n","    for article_pair in similar_pairs:\n","        article_a, article_b = article_pair\n","        print(f\"{article_a} {article_b}\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6343044,"sourceId":10254291,"sourceType":"datasetVersion"}],"dockerImageVersionId":30822,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":119.734628,"end_time":"2025-02-06T02:36:09.356588","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-06T02:34:09.62196","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}